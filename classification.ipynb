{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pranav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file = 'train.csv'\n",
    "test_csv_file = 'test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_csv_file)\n",
    "test_data = pd.read_csv(test_csv_file)\n",
    "\n",
    "dev_set_size = 7600\n",
    "train_set = train_data.iloc[dev_set_size:]\n",
    "dev_set = train_data.iloc[:dev_set_size]\n",
    "\n",
    "train_sentences = train_set['Description'].tolist()\n",
    "train_labels = train_set['Class Index'].tolist()\n",
    "dev_sentences = dev_set['Description'].tolist()\n",
    "dev_labels = dev_set['Class Index'].tolist()\n",
    "test_sentences = test_data['Description'].tolist()\n",
    "test_labels = test_data['Class Index'].tolist()\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "    tokens = sentence.split()\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train_sentences_cleaned = [clean_sentence(sentence) for sentence in train_sentences]\n",
    "dev_sentences_cleaned = [clean_sentence(sentence) for sentence in dev_sentences]\n",
    "test_sentences_cleaned = [clean_sentence(sentence) for sentence in test_sentences]\n",
    "\n",
    "train_sentences_tokenized = [word_tokenize(sentence) for sentence in train_sentences_cleaned]\n",
    "dev_sentences_tokenized = [word_tokenize(sentence) for sentence in dev_sentences_cleaned]\n",
    "test_sentences_tokenized = [word_tokenize(sentence) for sentence in test_sentences_cleaned]\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "length = []\n",
    "for i in range(len(train_data['Description'])):\n",
    "    sentence = re.findall(r\"[\\w']+|[.,!?;'-]\", train_data['Description'][i])\n",
    "    length.append(len(sentence))\n",
    "length.sort()\n",
    "max_seq_length = length[int(0.95*len(length))]+1\n",
    "\n",
    "\n",
    "def pad_sequences(sentences, max_length):\n",
    "    padded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) >= max_length:\n",
    "            padded_sentences.append(sentence[:max_length])\n",
    "        else:\n",
    "            padded_sentences.append(sentence + ['<PAD>'] * (max_length - len(sentence)))\n",
    "    return padded_sentences\n",
    "\n",
    "train_sentences_padded = pad_sequences(train_sentences_tokenized, max_seq_length)\n",
    "dev_sentences_padded = pad_sequences(dev_sentences_tokenized, max_seq_length)\n",
    "test_sentences_padded = pad_sequences(test_sentences_tokenized, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Sentences (Padded):\n",
      "[['such', 'an', 'embarrassment', 'of', 'riches', 'apple', 'macintosh', 'users', 'have', 'two', 'new', 'web', 'browsers', 'to', 'choose', 'from', 'it', 's', 'a', 'curious', 'market', 'all', 'macs', 'come', 'with', 'two', 'free', 'browsers', 'microsoft', 's', 'internet', 'explorer', 'and', 'apple', 's', 'own', 'slick', 'and', 'elegant', 'safari', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['the', 'women', 's', 'gold', 'medallist', 'in', 'shot', 'put', 'irina', 'korzhanenko', 'has', 'been', 'disqualified', 'from', 'the', 'olympics', 'and', 'stripped', 'of', 'her', 'medal', 'after', 'testing', 'positive', 'for', 'doping', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']]\n",
      "Train Set Labels:\n",
      "[4, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set Sentences (Padded):\")\n",
    "print(train_sentences_padded[:2])\n",
    "print(\"Train Set Labels:\")\n",
    "print(train_labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for sentence in train_sentences_tokenized for word in sentence]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "vocab = ['<UNK>', '<PAD>'] + [word for word, count in word_counts.items()]\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "def sentence_to_indices(sentence, word2idx):\n",
    "    return [word2idx.get(word, word2idx['<UNK>']) for word in sentence]\n",
    "\n",
    "train_indices_elmo = [sentence_to_indices(sentence, word2idx) for sentence in train_sentences_padded]\n",
    "dev_indices_elmo = [sentence_to_indices(sentence, word2idx) for sentence in dev_sentences_padded]\n",
    "test_indices_elmo = [sentence_to_indices(sentence, word2idx) for sentence in test_sentences_padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMo(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
    "        super(ELMo, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding Layer, Default- freeze = True\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "\n",
    "        # Forward Language Model\n",
    "        self.lstm_forward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm_forward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.linear_mode1 = nn.Linear(2*hidden_dim, vocab_size)\n",
    "\n",
    "        # Backward Language Model\n",
    "        self.lstm_backward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm_backward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.linear_mode2 = nn.Linear(2*hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        forward_embed = self.embedding(input_data)\n",
    "        forward_lstm1, _ = self.lstm_forward1(forward_embed)\n",
    "        forward_lstm2, _ = self.lstm_forward2(forward_lstm1)\n",
    "\n",
    "        input_data = torch.flip(input_data, dims=[1])\n",
    "        backward_embed = self.embedding(input_data)\n",
    "        backward_lstm1, _ = self.lstm_backward1(backward_embed)\n",
    "        backward_lstm2, _ = self.lstm_backward2(backward_lstm1)\n",
    "        \n",
    "        # Flip bacward_lstm words, Concatenate forward1 and backward1 outputs\n",
    "        backward_lstm1 = torch.flip(backward_lstm1, dims=[1])\n",
    "        lstm_concat1 = torch.cat((forward_lstm1, backward_lstm1), dim=-1)\n",
    "        \n",
    "        # Concatenate forward2 and backward2 outputs\n",
    "        backward_lstm2 = torch.flip(backward_lstm2, dims=[1])\n",
    "        lstm_concat2 = torch.cat((forward_lstm2, backward_lstm2), dim=-1)\n",
    "\n",
    "        # Concatenate forward and backward embeddings word by word\n",
    "        embedding_concat = torch.cat((forward_embed, forward_embed), dim=-1)\n",
    "        \n",
    "        return embedding_concat, lstm_concat1, lstm_concat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "\n",
    "elmo = ELMo(vocab_size, embedding_dim, hidden_dim, torch.rand(vocab_size, embedding_dim)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ELMo(\n",
       "  (embedding): Embedding(81098, 100)\n",
       "  (lstm_forward1): LSTM(100, 100, batch_first=True)\n",
       "  (lstm_forward2): LSTM(100, 100, batch_first=True)\n",
       "  (linear_mode1): Linear(in_features=200, out_features=81098, bias=True)\n",
       "  (lstm_backward1): LSTM(100, 100, batch_first=True)\n",
       "  (lstm_backward2): LSTM(100, 100, batch_first=True)\n",
       "  (linear_mode2): Linear(in_features=200, out_features=81098, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo.load_state_dict(torch.load('pretrained_elmo.zip', map_location=device))\n",
    "elmo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_elmo = torch.tensor(train_indices_elmo, dtype=torch.long)\n",
    "dev_data_elmo = torch.tensor(dev_indices_elmo, dtype=torch.long)\n",
    "test_data_elmo = torch.tensor(test_indices_elmo, dtype=torch.long)\n",
    "\n",
    "train_data_with_labels = TensorDataset(train_data_elmo[:100000], torch.tensor(train_labels[:100000], dtype=torch.long))\n",
    "dev_data_with_labels = TensorDataset(dev_data_elmo[:5000], torch.tensor(dev_labels[:5000], dtype=torch.long))\n",
    "test_data_with_labels = TensorDataset(test_data_elmo[:5000], torch.tensor(test_labels[:5000], dtype=torch.long))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data_with_labels, batch_size=batch_size, shuffle=False)\n",
    "dev_loader = DataLoader(dev_data_with_labels, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data_with_labels, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
      "         20, 21, 22, 23, 24, 25, 26, 11, 27, 14, 28, 19, 29, 30, 31,  7, 19, 32,\n",
      "         33, 31, 34, 35,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1]]), tensor([4]))\n"
     ]
    }
   ],
   "source": [
    "print(train_data_with_labels[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers = 1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=False, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.weights = nn.Parameter(torch.tensor([0.33, 0.33, 0.33], dtype=torch.float32))\n",
    "        self.gamma = nn.Parameter(torch.tensor(1, dtype=torch.float32))\n",
    "\n",
    "\n",
    "    def forward(self, embedding1, embedding2, embedding3):\n",
    "        weights = nn.functional.softmax(self.weights, dim = 0)\n",
    "        x = (embedding3*weights[0] + embedding1*weights[1] + embedding2*weights[2])*self.gamma\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))                            # Initialize hidden state\n",
    "        output_last = output[:, -1]                                          # Take the output from the last time step\n",
    "        output_fc = self.fc(output_last)                                     # Pass it through the fully connected layer\n",
    "        return output_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = train_data['Class Index'].unique().shape[0]\n",
    "embedding_dim = 200\n",
    "hidden_dim = 200\n",
    "\n",
    "lstm = LSTM(output_dim, embedding_dim, hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(model,elmo_model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    elmo_model.eval()\n",
    "\n",
    "    for input_seq, target_seq in tqdm(train_loader):\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq - 1\n",
    "        target_seq = target_seq.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embedding1, embedding2, embedding3 = elmo_model(input_seq)\n",
    "\n",
    "        outputs = model(embedding1, embedding2, embedding3)\n",
    "        loss = criterion(outputs, target_seq) \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predicted = torch.argmax(outputs, dim = 1)\n",
    "        correct += torch.sum(target_seq == predicted).item()\n",
    "        total += target_seq.shape[0]\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm(model, elmo_model, dev_loader):\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in tqdm(dev_loader):\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq - 1\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            embedding1, embedding2, embedding3 = elmo_model(input_seq)\n",
    "\n",
    "            outputs = model(embedding1, embedding2, embedding3)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_labels.extend(target_seq.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "    return true_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainable Lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)*100\n",
    "    precision = precision_score(predicted_labels, true_labels, average='weighted')*100\n",
    "    recall = recall_score(predicted_labels, true_labels, average='weighted')*100\n",
    "    f1 = f1_score(predicted_labels, true_labels, average='weighted')*100\n",
    "    confusion = confusion_matrix(predicted_labels, true_labels)\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(f\"Recall = {recall}\")\n",
    "    print(f\"F1-Score = {f1}\")\n",
    "    print(f\"Precision = {precision}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion}\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:52<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Train) - Loss: 0.6956 Training Set Accuracy 69.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:43<00:00, 19.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Train) - Loss: 0.3190 Training Set Accuracy 88.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:59<00:00, 17.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Train) - Loss: 0.2702 Training Set Accuracy 90.593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:51<00:00, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Train) - Loss: 0.2339 Training Set Accuracy 91.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:54<00:00, 17.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Train) - Loss: 0.1995 Training Set Accuracy 93.189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:58<00:00, 17.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Train) - Loss: 0.1681 Training Set Accuracy 94.316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:48<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Train) - Loss: 0.1420 Training Set Accuracy 95.185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:50<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 (Train) - Loss: 0.1188 Training Set Accuracy 96.054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:52<00:00, 18.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 (Train) - Loss: 0.1024 Training Set Accuracy 96.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:23<00:00, 21.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 (Train) - Loss: 0.0891 Training Set Accuracy 97.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr = 1e-3)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, accuracy = train_lstm(lstm, elmo, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} (Train) - Loss: {train_loss:.4f} Training Set Accuracy {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2493, 0.3161, 0.4346], grad_fn=<SoftmaxBackward0>),\n",
       " Parameter containing:\n",
       " tensor(1.6485, requires_grad=True))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.softmax(lstm.weights, dim = 0), lstm.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm.state_dict(), 'classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 68.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 89.78\n",
      "Recall = 89.78\n",
      "F1-Score = 89.79872608594803\n",
      "Precision = 89.94152474201607\n",
      "Confusion Matrix: \n",
      "[[1124   10   30   35]\n",
      " [  50 1243   12   13]\n",
      " [  64    9 1011   81]\n",
      " [  48    8  151 1111]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels, predicted_labels = evaluate_lstm(lstm, elmo, test_loader)\n",
    "evaluation_metrics(true_labels=true_labels, predicted_labels=predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frozen Lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.7442, 0.1868, 0.0690], requires_grad=True) Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "For Initial Weights = tensor([0.7442, 0.1868, 0.0690])\n",
      "\n",
      "\n",
      "Training the Model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:23<00:00, 21.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Train) - Loss: 0.7529 Training Set Accuracy 67.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:22<00:00, 21.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Train) - Loss: 0.3443 Training Set Accuracy 87.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:19<00:00, 22.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Train) - Loss: 0.2972 Training Set Accuracy 89.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:20<00:00, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Train) - Loss: 0.2659 Training Set Accuracy 90.59700000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:22<00:00, 21.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Train) - Loss: 0.2381 Training Set Accuracy 91.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:23<00:00, 21.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Train) - Loss: 0.2110 Training Set Accuracy 92.54599999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:24<00:00, 21.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Train) - Loss: 0.1860 Training Set Accuracy 93.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:22<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 (Train) - Loss: 0.1637 Training Set Accuracy 94.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:21<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 (Train) - Loss: 0.1455 Training Set Accuracy 94.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:21<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 (Train) - Loss: 0.1298 Training Set Accuracy 95.50699999999999\n",
      "Parameter containing:\n",
      "tensor([0.7442, 0.1868, 0.0690]) Parameter containing:\n",
      "tensor(1.)\n",
      "Testing the Model on Test Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 60.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 88.78\n",
      "Recall = 88.78\n",
      "F1-Score = 88.80254126888259\n",
      "Precision = 88.9050533896645\n",
      "Confusion Matrix: \n",
      "[[1131   18   43   43]\n",
      " [  47 1229   15   18]\n",
      " [  57    9  992   92]\n",
      " [  51   14  154 1087]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_dim = train_data['Class Index'].unique().shape[0]\n",
    "embedding_dim = 200\n",
    "hidden_dim = 200\n",
    "num_epochs = 10\n",
    "\n",
    "lstm1 = LSTM(output_dim, embedding_dim, hidden_dim).to(device)\n",
    "initial_weights = torch.randn(3, dtype = torch.float32, device = device)\n",
    "initial_weights = nn.functional.softmax(initial_weights, dim = 0)\n",
    "lstm1.weights = torch.nn.Parameter(initial_weights)\n",
    "print(lstm1.weights, lstm1.gamma)\n",
    "\n",
    "# Freeze the weights\n",
    "lstm1.weights.requires_grad = False\n",
    "lstm1.gamma.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm1.parameters(), lr = 1e-3)\n",
    "\n",
    "\n",
    "print(f\"For Initial Weights = {initial_weights}\\n\\n\")\n",
    "\n",
    "print(\"Training the Model\\n\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, accuracy = train_lstm(lstm1, elmo, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} (Train) - Loss: {train_loss:.4f} Training Set Accuracy {accuracy*100}\")\n",
    "\n",
    "print(lstm1.weights, lstm1.gamma)\n",
    "\n",
    "print(\"Testing the Model on Test Set\")\n",
    "true_labels, predicted_labels = evaluate_lstm(lstm1, elmo, test_loader)\n",
    "evaluation_metrics(true_labels=true_labels, predicted_labels=predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [00:49<00:00, 62.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 94.829\n",
      "Recall = 94.829\n",
      "F1-Score = 94.83753983076268\n",
      "Precision = 94.93347362900482\n",
      "Confusion Matrix: \n",
      "[[23281    50   284   272]\n",
      " [  481 24875   152   131]\n",
      " [  743    42 22599   582]\n",
      " [  527    53  1854 24074]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels, predicted_labels = evaluate_lstm(lstm, elmo, train_loader)\n",
    "evaluation_metrics(true_labels=true_labels, predicted_labels=predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [00:52<00:00, 59.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.42\n",
      "Recall = 93.42\n",
      "F1-Score = 93.42824243025008\n",
      "Precision = 93.54659342881327\n",
      "Confusion Matrix: \n",
      "[[23447   177   506   488]\n",
      " [  352 24630   162   146]\n",
      " [  625    91 21730   812]\n",
      " [  608   122  2491 23613]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels, predicted_labels = evaluate_lstm(lstm1, elmo, train_loader)\n",
    "evaluation_metrics(true_labels=true_labels, predicted_labels=predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learnable Function** - The Input Sequence is passed to the ELMO model for the generation of the 3 embeddings (Forward, Backward and Glove Embeddings). Then, simply a Linear Combination of Vectors obtained is done with the appropriate Trained scaling factor to create the Final Contextualized Embedding. So, the final mathematical representation of the Embeddings are as follows:\n",
    "\n",
    "Forward, Backward, Glove Embeddings = ELMO Model(Input Sequnce)\n",
    "\n",
    "Contextualized Embedding = (0.2493**Forward** + 0.3161**Backward** + 0.4346**Glove Embeddings**)1.6485 for Trainable Weights Case.\n",
    "\n",
    "Contextualized Embedding = (0.7442**Forward** + 0.1868**Backward** + 0.0690**Glove Embeddings**)1 for Frozen Weights Case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracies = [69.708, 88.78, 90.593, 91.955, 93.189, 94.316, 95.185, 96.054, 96.599, 97.024]\n",
    "epochs = np.arange(10)+1\n",
    "plt.scatter(accuracies, epochs)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = [67.105, 87.864, 89.483, 90.59700000000001, 91.582, 92.54599999999999, 93.506, 94.337, 94.931, 95.50699999999999]\n",
    "epochs = np.arange(10)+1\n",
    "plt.scatter(accuracies, epochs, color = 'pink')\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
